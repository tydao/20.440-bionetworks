{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "outer-return",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "### Initializes the model architectures and functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "athletic-plane",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import anndata\n",
    "from scipy.spatial import distance\n",
    "from sklearn.model_selection import train_test_split\n",
    "import scanpy as sc\n",
    "from collections import OrderedDict\n",
    "from collections import defaultdict\n",
    "import sys\n",
    "import MulticoreTSNE\n",
    "from anndata import AnnData\n",
    "from matplotlib import pyplot as plt\n",
    "import matplotlib as mpl\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "clean-making",
   "metadata": {
    "code_folding": [],
    "hidden": true
   },
   "outputs": [],
   "source": [
    "import argparse\n",
    "\n",
    "def get_parser():\n",
    "    parser = argparse.ArgumentParser()\n",
    "    \n",
    "    parser.add_argument('-pretrain_batch', '--pretrain_batch',\n",
    "                        type=int,\n",
    "                        help='Batch size for pretraining. Default: no batch',\n",
    "                        default=None)\n",
    "    \n",
    "    parser.add_argument('-pretrain','--pretrain',\n",
    "                        type = bool,\n",
    "                        default = True,\n",
    "                        help='Pretrain model with autoencoder; otherwise load existing')\n",
    "    \n",
    "    parser.add_argument('-nepoch', '--epochs',\n",
    "                        type=int,\n",
    "                        help='number of epochs to train for',\n",
    "                        default=30)\n",
    "\n",
    "    parser.add_argument('-nepoch_pretrain', '--epochs_pretrain',\n",
    "                        type=int,\n",
    "                        help='number of epochs to pretrain for',\n",
    "                        default=25)\n",
    "\n",
    "    parser.add_argument('-source_file','--model_file',\n",
    "                        type = str,\n",
    "                        default = 'trained_models/source.pt',\n",
    "                        help='location for storing source model and data')\n",
    "\n",
    "    parser.add_argument('-lr', '--learning_rate',\n",
    "                        type=float,\n",
    "                        help='learning rate for the model, default=0.001',\n",
    "                        default=0.001)\n",
    "\n",
    "    parser.add_argument('-lrS', '--lr_scheduler_step',\n",
    "                        type=int,\n",
    "                        help='StepLR learning rate scheduler step, default=20',\n",
    "                        default=20) \n",
    "\n",
    "    parser.add_argument('-lrG', '--lr_scheduler_gamma',\n",
    "                        type=float,\n",
    "                        help='StepLR learning rate scheduler gamma, default=0.5',\n",
    "                        default=0.5)\n",
    "  \n",
    "    parser.add_argument('-seed', '--manual_seed',\n",
    "                        type=int,\n",
    "                        help='input for the manual seeds initializations',\n",
    "                        default=3)\n",
    "    \n",
    "    parser.add_argument('--cuda',\n",
    "                        action='store_true',\n",
    "                        help='enables cuda')\n",
    "    \n",
    "    return parser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "measured-portrait",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# coding=utf-8\n",
    "import torch.utils.data as data\n",
    "import numpy as np\n",
    "import torch\n",
    "\n",
    "'''\n",
    "Class representing dataset for an single-cell experiment.\n",
    "'''\n",
    "\n",
    "IMG_CACHE = {}\n",
    "\n",
    "\n",
    "class ExperimentDataset(data.Dataset):\n",
    "    \n",
    "    \n",
    "    def __init__(self, x, cells, genes, metadata, y=[]):\n",
    "        '''\n",
    "        x: numpy array of gene expressions of cells (rows are cells)\n",
    "        cells: cell IDs in the order of appearance\n",
    "        genes: gene IDs in the order of appearance\n",
    "        metadata: experiment identifier\n",
    "        y: numeric labels of cells (empty list if unknown)\n",
    "        '''\n",
    "        super(ExperimentDataset, self).__init__()\n",
    "        \n",
    "        self.nitems = x.shape[0]\n",
    "        if len(y)>0:\n",
    "            print(\"== Dataset: Found %d items \" % x.shape[0])\n",
    "            print(\"== Dataset: Found %d classes\" % len(np.unique(y)))\n",
    "                \n",
    "        if type(x)==torch.Tensor:\n",
    "            self.x = x\n",
    "        else:\n",
    "            shape = x.shape[1]\n",
    "            self.x = [torch.from_numpy(inst).view(shape).float() for inst in x]\n",
    "        if len(y)==0:\n",
    "            y = np.zeros(len(self.x), dtype=np.int64)\n",
    "        self.y = tuple(y.tolist())\n",
    "        self.xIDs = cells\n",
    "        self.yIDs = genes\n",
    "        self.metadata = metadata\n",
    "            \n",
    "    def __getitem__(self, idx):\n",
    "        return self.x[idx].squeeze(), self.y[idx], self.xIDs[idx]\n",
    "    #, self.yIDs[idx]\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.nitems\n",
    "    \n",
    "    def get_dim(self):\n",
    "        return self.x[0].shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ideal-darkness",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "class EpochSampler(object):\n",
    "    '''\n",
    "    EpochSampler: yield permuted indexes at each epoch.\n",
    "   \n",
    "    __len__ returns the number of episodes per epoch (same as 'self.iterations').\n",
    "    '''\n",
    "\n",
    "    def __init__(self, indices):\n",
    "        '''\n",
    "        Initialize the EpochSampler object\n",
    "        Args:\n",
    "        - labels: an iterable containing all the labels for the current dataset\n",
    "        samples indexes will be infered from this iterable.\n",
    "        - iterations: number of epochs\n",
    "        '''\n",
    "        super(EpochSampler, self).__init__()\n",
    "        self.indices = indices\n",
    "        \n",
    "\n",
    "    def __iter__(self):\n",
    "        '''\n",
    "        yield a batch of indexes\n",
    "        '''\n",
    "        \n",
    "        while(True):\n",
    "            shuffled_idx = self.indices[torch.randperm(len(self.indices))]\n",
    "            \n",
    "            yield shuffled_idx\n",
    "            \n",
    "\n",
    "    def __len__(self):\n",
    "        '''\n",
    "        returns the number of iterations (episodes) per epoch\n",
    "        '''\n",
    "        return self.iterations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "anticipated-skating",
   "metadata": {
    "code_folding": [],
    "hidden": true
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import DataLoader ##prefetch by batch\n",
    "#from model.epoch_sampler import EpochSampler\n",
    "\n",
    "\n",
    "def init_labeled_loader(data, val_split = 0.8):\n",
    "    \"\"\"Initialize loaders for train and validation sets. \n",
    "    Class labels are used only\n",
    "    for stratified sampling between train and validation set.\"\"\"\n",
    "    \n",
    "    target = torch.tensor(list(data.y))\n",
    "    uniq = torch.unique(target, sorted=True)\n",
    "    \n",
    "    class_idxs = list(map(lambda c: target.eq(c).nonzero(), uniq))\n",
    "    class_idxs = [idx[torch.randperm(len(idx))] for idx in class_idxs]\n",
    "    \n",
    "    train_idx = torch.cat([idx[:int(val_split*len(idx))] for idx in class_idxs])\n",
    "    val_idx = torch.cat([idx[int(val_split*len(idx)):] for idx in class_idxs])\n",
    "    \n",
    "    train_loader = DataLoader(data, \n",
    "                              batch_sampler=EpochSampler(train_idx),\n",
    "                              pin_memory=True)\n",
    "    \n",
    "    val_loader = DataLoader(data, \n",
    "                            batch_sampler=EpochSampler(val_idx),\n",
    "                            pin_memory=True)\n",
    "    \n",
    "    return train_loader, val_loader\n",
    "\n",
    "\n",
    "def init_loader(datasets, val_split = 0.8):\n",
    "    \n",
    "    train_loader_all = []\n",
    "    val_loader_all = []\n",
    "    \n",
    "    for data in datasets:\n",
    "        \n",
    "        curr_load_tr, curr_load_val = init_labeled_loader(data, val_split)\n",
    "        train_loader_all.append(curr_load_tr)\n",
    "        val_loader_all.append(curr_load_val)\n",
    "    \n",
    "    if val_split==1:\n",
    "        val_loader_all = None\n",
    "        \n",
    "    return train_loader_all, val_loader_all\n",
    "\n",
    "\n",
    "def init_data_loaders(labeled_data, unlabeled_data, \n",
    "                      pretrain_data, pretrain_batch, val_split):\n",
    "    \n",
    "    \"\"\"Initialize loaders for pretraing, \n",
    "    training (labeled and unlabeled datasets) and validation. \"\"\"\n",
    "    \n",
    "    train_loader, val_loader = init_loader(labeled_data, val_split)\n",
    "    \n",
    "    if not pretrain_data:\n",
    "        pretrain_data = unlabeled_data\n",
    "    \n",
    "    pretrain_loader = torch.utils.data.DataLoader(dataset=pretrain_data, shuffle=True,\n",
    "                                                  batch_size=pretrain_batch if pretrain_batch!=None else len(unlabeled_data.x))        \n",
    "    test_loader = DataLoader(unlabeled_data, \n",
    "                            batch_sampler=EpochSampler(torch.randperm(len(unlabeled_data.x))),\n",
    "                            pin_memory=True) \n",
    "    \n",
    "    #test_loader,_ = init_loader([unlabeled_data], 1.0) # to reproduce results in the paper\n",
    "    #test_loader = test_loader[0]\n",
    "    return train_loader, test_loader, pretrain_loader, val_loader\n",
    "           \n",
    "           \n",
    "def euclidean_dist(x, y):\n",
    "    '''\n",
    "    Compute euclidean distance between two tensors\n",
    "    '''\n",
    "    # x: N x D\n",
    "    # y: M x D\n",
    "    n = x.size(0)\n",
    "    m = y.size(0)\n",
    "    d = x.size(1)\n",
    "    if d != y.size(1):\n",
    "        raise Exception\n",
    "\n",
    "    x = x.unsqueeze(1).expand(n, m, d)\n",
    "    y = y.unsqueeze(0).expand(n, m, d)\n",
    "\n",
    "    return torch.pow(x - y, 2).sum(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "equipped-permit",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "\n",
    "def full_block(in_features, out_features, p_drop):\n",
    "        return nn.Sequential(\n",
    "            nn.Linear(in_features, out_features, bias=True),\n",
    "            nn.LayerNorm(out_features),\n",
    "            nn.ELU(),\n",
    "            nn.Dropout(p=p_drop),\n",
    "        )\n",
    "\n",
    "class FullNet(nn.Module):\n",
    "    '''\n",
    "    '''\n",
    "    def __init__(self, x_dim, hid_dim=64, z_dim=64, p_drop=0.2):\n",
    "        super(FullNet, self).__init__()\n",
    "        self.z_dim = z_dim\n",
    "        \n",
    "        self.encoder = nn.Sequential(\n",
    "            full_block(x_dim, hid_dim, p_drop),\n",
    "            full_block(hid_dim, z_dim, p_drop),\n",
    "        )\n",
    "        \n",
    "        self.decoder = nn.Sequential(\n",
    "            full_block(z_dim, hid_dim, p_drop),\n",
    "            full_block(hid_dim, x_dim, p_drop),\n",
    "        )\n",
    "      \n",
    "    def forward(self, x):\n",
    "        \n",
    "        encoded = self.encoder(x)\n",
    "        decoded = self.decoder(encoded)\n",
    "        \n",
    "        return encoded, decoded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "therapeutic-fishing",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.nn import functional as F\n",
    "\n",
    "#from model.utils import euclidean_dist\n",
    "\n",
    "def loss_task(encoded, prototypes, target, criterion='dist'):\n",
    "    \"\"\"Calculate loss.\n",
    "    criterion: NNLoss - assign to closest prototype and calculate NNLoss\n",
    "    dist - loss is distance to prototype that example needs to be assigned to\n",
    "                and -distance to prototypes from other class\n",
    "    \"\"\"\n",
    "    \n",
    "    uniq = torch.unique(target, sorted=True)\n",
    "    \n",
    "    ###index of samples for each class of labels\n",
    "    class_idxs = list(map(lambda c: target.eq(c).nonzero(), uniq))\n",
    "    \n",
    "    # prepare targets so they start from 0,1\n",
    "    for idx,v in enumerate(uniq):\n",
    "        target[target==v]=idx\n",
    "    \n",
    "    dists = euclidean_dist(encoded, prototypes)\n",
    "    \n",
    "    if criterion=='NNLoss':\n",
    "       \n",
    "        loss = torch.nn.NLLLoss()\n",
    "        log_p_y = F.log_softmax(-dists, dim=1)\n",
    "        \n",
    "        loss_val = loss(log_p_y, target)\n",
    "        _, y_hat = log_p_y.max(1)\n",
    "        \n",
    "    \n",
    "    elif criterion=='dist':\n",
    "        \n",
    "        loss_val = torch.stack([dists[idx_example, idx_proto].mean(0) for idx_proto,idx_example in enumerate(class_idxs)]).mean()\n",
    "        #loss_val1 = loss_val1/len(embeddings) \n",
    "        y_hat = torch.max(-dists,1)[1]\n",
    "        \n",
    "    acc_val = y_hat.eq(target.squeeze()).float().mean()    \n",
    "        \n",
    "    return loss_val, acc_val\n",
    "\n",
    "def loss_test_nn(encoded, prototypes):\n",
    "    dists = euclidean_dist(encoded, prototypes)\n",
    "    min_dist = torch.min(dists, 1)\n",
    "    \n",
    "    y_hat = min_dist[1]\n",
    "    args_uniq = torch.unique(y_hat, sorted=True)\n",
    "    args_count = torch.stack([(y_hat==x_u).sum() for x_u in args_uniq])\n",
    "    print(args_count)\n",
    "    \n",
    "    loss = torch.nn.NLLLoss()\n",
    "    log_p_y = F.log_softmax(-dists, dim=1)\n",
    "    print(log_p_y.shape)\n",
    "        \n",
    "    loss_val = loss(log_p_y, y_hat)\n",
    "    _, y_hat = log_p_y.max(1)\n",
    "    \n",
    "    return loss_val, args_count\n",
    "\n",
    "\n",
    "def loss_test_basic(encoded, prototypes):\n",
    "    dists = euclidean_dist(encoded, prototypes)\n",
    "    min_dist = torch.min(dists, 1)\n",
    "    \n",
    "    y_hat = min_dist[1]\n",
    "    args_uniq = torch.unique(y_hat, sorted=True)\n",
    "    args_count = torch.stack([(y_hat==x_u).sum() for x_u in args_uniq])\n",
    "    #print(args_count)\n",
    "    \n",
    "    min_dist = min_dist[0] # get_distances\n",
    "    \n",
    "    #thr = torch.stack([torch.sort(min_dist[y_hat==idx_class])[0][int(len(min_dist[y_hat==idx_class])*0.9)] for idx_class in args_uniq])\n",
    "    #loss_val = torch.stack([min_dist[y_hat==idx_class][min_dist[y_hat==idx_class]>=thr[idx_class]].mean(0) for idx_class in args_uniq]).mean()\n",
    "    \n",
    "    loss_val = torch.stack([min_dist[y_hat==idx_class].mean(0) for idx_class in args_uniq]).mean()\n",
    "    \n",
    "    #loss_val,_ = loss_task(encoded, prototypes, y_hat, criterion='dist') # same\n",
    "    \n",
    "    return loss_val, args_count\n",
    "\n",
    "def loss_test(encoded, prototypes, tau):\n",
    "    #prototypes = torch.stack(prototypes).squeeze() \n",
    "    loss_val_test, args_count = loss_test_basic(encoded, prototypes)\n",
    "    \n",
    "    \n",
    "    ###Intracluster distance \n",
    "    if tau>0:\n",
    "        dists = euclidean_dist(prototypes, prototypes)\n",
    "        nproto = prototypes.shape[0]\n",
    "        loss_val2 = - torch.sum(dists)/(nproto*nproto-nproto)\n",
    "        \n",
    "        loss_val_test += tau*loss_val2\n",
    "        \n",
    "    return loss_val_test, args_count\n",
    "\n",
    "def reconstruction_loss(decoded, x):\n",
    "    loss_func = torch.nn.MSELoss()\n",
    "    loss_rcn = loss_func(decoded, x)\n",
    "    #print('Reconstruction {}'.format(loss_rcn))\n",
    "    \n",
    "    return loss_rcn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "antique-tiger",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "#from sklearn.cluster import k_means_\n",
    "from sklearn.cluster import KMeans\n",
    "\n",
    "#############################################\n",
    "#############################################\n",
    "def compute_landmarks_tr(embeddings, target, prev_landmarks=None, tau=0.2):\n",
    "    \"\"\"Computing landmarks of each class in the labeled meta-dataset. \n",
    "    \n",
    "    Landmark is a closed form solution of \n",
    "    minimizing distance to the mean and maximizing distance to other landmarks. \n",
    "    \n",
    "    If tau=0, landmarks are just mean of data points.\n",
    "    \n",
    "    embeddings: embeddings of the labeled dataset\n",
    "    target: labels in the labeled dataset\n",
    "    prev_landmarks: landmarks from previous iteration\n",
    "    tau: regularizer for inter- and intra-cluster distance\n",
    "    \"\"\"\n",
    "    \n",
    "    uniq = torch.unique(target, sorted=True)\n",
    "    class_idxs = list(map(lambda c: target.eq(c).nonzero(), uniq))\n",
    "    \n",
    "    landmarks_mean = torch.stack([embeddings[idx_class].mean(0) for idx_class in class_idxs]).squeeze()\n",
    "    \n",
    "    if prev_landmarks is None or tau==0:\n",
    "        return landmarks_mean\n",
    "    \n",
    "    suma = prev_landmarks.sum(0)\n",
    "    nlndmk = prev_landmarks.shape[0]\n",
    "    lndmk_dist_part = (tau/(nlndmk-1))*torch.stack([suma-p for p in prev_landmarks])\n",
    "    landmarks = 1/(1-tau)*(landmarks_mean-lndmk_dist_part)\n",
    "    \n",
    "    return landmarks\n",
    "\n",
    "#############################################\n",
    "#############################################\n",
    "\n",
    "\n",
    "def init_landmarks(n_clusters, tr_load, test_load, model, device, mode='kmeans', pretrain=True):\n",
    "    \"\"\"Initialization of landmarks of the labeled and unlabeled meta-dataset.\n",
    "    nclusters: number of expected clusters in the unlabeled meta-dataset\n",
    "    tr_load: data loader for labeled meta-dataset\n",
    "    test_load: data loader for unlabeled meta-dataset\n",
    "    \"\"\"\n",
    "    lndmk_tr = [torch.zeros(size=(len(np.unique(dl.dataset.y)), model.z_dim), \n",
    "                            requires_grad=True, device=device) for dl in tr_load]\n",
    "    \n",
    "    lndmk_test = [torch.zeros(size=(1, model.z_dim), \n",
    "                              requires_grad=True, device=device) \n",
    "                       for _ in range(n_clusters)]\n",
    "    \n",
    "    kmeans_init_tr = [init_step(dl.dataset, model, device, pretrained=pretrain, mode=mode) \n",
    "                      for dl in tr_load]\n",
    "    \n",
    "    kmeans_init_test = init_step(test_load.dataset, model, device, \n",
    "                                 pretrained=pretrain, mode=mode, \n",
    "                                 n_clusters=n_clusters)\n",
    "    \n",
    "    ##No gradient calculation\n",
    "    with torch.no_grad():\n",
    "        [lndmk.copy_(kmeans_init_tr[idx])  for idx,lndmk in enumerate(lndmk_tr)]\n",
    "        [lndmk_test[i].copy_(kmeans_init_test[i,:]) for i in range(kmeans_init_test.shape[0])]\n",
    "        \n",
    "    return lndmk_tr, lndmk_test\n",
    "\n",
    "\n",
    "def init_step(dataset, model, device, pretrained, mode='kmeans',n_clusters=None):\n",
    "    \"\"\"Initialization of landmarks with k-means or k-means++ given dataset.\"\"\"\n",
    "    \n",
    "    if n_clusters==None:\n",
    "        n_clusters = len(np.unique(dataset.y))\n",
    "    nexamples = len(dataset.x)\n",
    "        \n",
    "    X =  torch.stack([dataset.x[i] for i in range(nexamples)])\n",
    "    \n",
    "    if mode=='kmeans++':\n",
    "        if not pretrained: # find centroids in original space\n",
    "            landmarks = k_means_._init_centroids(X.cpu().numpy(), n_clusters, 'k-means++')\n",
    "            landmarks = torch.tensor(landmarks, device=device)\n",
    "            landmarks = landmarks.to(device)\n",
    "            lndmk_encoded,_ = model(landmarks)\n",
    "            \n",
    "        else:\n",
    "            X = X.to(device)\n",
    "            encoded,_ = model(X)\n",
    "            landmarks = k_means_._init_centroids(encoded.data.cpu().numpy(), n_clusters, 'k-means++')\n",
    "            lndmk_encoded = torch.tensor(landmarks, device=device)\n",
    "    \n",
    "    elif mode=='kmeans': # run kmeans clustering\n",
    "        if not pretrained: \n",
    "            kmeans = KMeans(n_clusters, random_state=0).fit(X.cpu().numpy())\n",
    "            landmarks = torch.tensor(kmeans.cluster_centers_, device=device)\n",
    "            landmarks = landmarks.to(device)\n",
    "            ##Feed forward net on landmarks  (k means cluster)\n",
    "            ##landmarks are k means cluster centers = coordinates of cluster center\n",
    "            lndmk_encoded,_ = model(landmarks)\n",
    "        \n",
    "        ##already pretrained; return coordinates of centers \n",
    "        else:\n",
    "            X = X.to(device)\n",
    "            encoded,_ = model(X)\n",
    "            kmeans = KMeans(n_clusters, random_state=0).fit(encoded.data.cpu().numpy())\n",
    "            lndmk_encoded = torch.tensor(kmeans.cluster_centers_, device=device)\n",
    "    \n",
    "    return lndmk_encoded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "noble-raleigh",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import sklearn.metrics as metrics\n",
    "from scipy.optimize import linear_sum_assignment \n",
    "\n",
    "def compute_scores(y_true, y_pred, scoring={'accuracy','precision','recall','nmi',\n",
    "                                                'adj_rand','f1_score','adj_mi'}):\n",
    "    y_true = y_true.cpu().numpy()\n",
    "    y_pred = y_pred.cpu().numpy()\n",
    "    \n",
    "    scores = {}\n",
    "    y_true, y_pred = hungarian_match(y_true, y_pred)\n",
    "    set_scores(scores, y_true, y_pred, scoring)\n",
    "        \n",
    "    return scores\n",
    "\n",
    "\n",
    "def set_scores(scores, y_true, y_pred, scoring):\n",
    "    labels=list(set(y_true))\n",
    "    \n",
    "    for metric in scoring:\n",
    "        if metric=='accuracy':\n",
    "            scores[metric] = metrics.accuracy_score(y_true, y_pred)\n",
    "        elif metric=='precision':\n",
    "            scores[metric] = metrics.precision_score(y_true, y_pred, labels, average='macro')\n",
    "        elif metric=='recall':\n",
    "            scores[metric] = metrics.recall_score(y_true, y_pred, labels, average='macro')\n",
    "        elif metric=='f1_score':\n",
    "            scores[metric] = metrics.f1_score(y_true, y_pred, labels, average='macro')\n",
    "        elif metric=='nmi':\n",
    "            scores[metric] = metrics.normalized_mutual_info_score(y_true, y_pred)\n",
    "        elif metric=='adj_mi':\n",
    "            scores[metric] = metrics.adjusted_mutual_info_score(y_true, y_pred)\n",
    "        elif metric=='adj_rand':\n",
    "            scores[metric] = metrics.adjusted_rand_score(y_true, y_pred)\n",
    "                \n",
    "                \n",
    "def hungarian_match(y_true, y_pred):\n",
    "    \"\"\"Matches predicted labels to original using hungarian algorithm.\"\"\"\n",
    "    \n",
    "    y_true = adjust_range(y_true)\n",
    "    y_pred = adjust_range(y_pred)\n",
    "    \n",
    "    D = max(y_pred.max(), y_true.max()) + 1\n",
    "    w = np.zeros((D, D), dtype=np.int64)\n",
    "    # Confusion matrix.\n",
    "    for i in range(y_pred.size):\n",
    "        w[y_pred[i], y_true[i]] += 1\n",
    "    ind = linear_sum_assignment(-w)\n",
    "    ind = np.asarray(ind)\n",
    "    ind = np.transpose(ind)\n",
    "    d = {i:j for i, j in ind}\n",
    "    y_pred = np.array([d[v] for v in y_pred])\n",
    "    \n",
    "    return y_true, y_pred\n",
    "\n",
    "\n",
    "def adjust_range(y):\n",
    "    \"\"\"Assures that the range of indices if from 0 to n-1.\"\"\"\n",
    "    y = np.array(y, dtype=np.int64)\n",
    "    val_set = set(y)\n",
    "    mapping = {val:i for  i,val in enumerate(val_set)}\n",
    "    y = np.array([mapping[val] for val in y], dtype=np.int64)\n",
    "    return y\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dressed-daisy",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "class MARS:\n",
    "    def __init__(self, n_clusters, params, \n",
    "                 labeled_data, unlabeled_data, \n",
    "                 pretrain_data=None, \n",
    "                 val_split=1.0, hid_dim_1=1000, hid_dim_2=100, p_drop=0.0, tau=0.2):\n",
    "        \"\"\"Initialization of MARS.\n",
    "        n_clusters: number of clusters in the unlabeled meta-dataset\n",
    "        params: parameters of the MARS model\n",
    "        labeled_data: list of labeled datasets. Each dataset needs to be instance of CellDataset.\n",
    "        unlabeled_data: unlabeled dataset. Instance of CellDataset.\n",
    "        pretrain_data: dataset for pretraining MARS. Instance of CellDataset. If not specified, unlabeled_data\n",
    "                        will be used.\n",
    "        val_split: percentage of data to use for train/val split (default: 1, meaning no validation set)\n",
    "        hid_dim_1: dimension in the first layer of the network (default: 1000)\n",
    "        hid_dim_2: dimension in the second layer of the network (default: 100)\n",
    "        p_drop: dropout probability (default: 0)\n",
    "        tau: regularizer for inter-cluster distance\n",
    "        \"\"\"\n",
    "        train_load, test_load, pretrain_load, val_load = init_data_loaders(labeled_data, unlabeled_data, \n",
    "                                                                           pretrain_data, params.pretrain_batch, \n",
    "                                                                           val_split)\n",
    "        self.train_loader = train_load\n",
    "        self.test_loader = test_load\n",
    "        self.pretrain_loader = pretrain_load\n",
    "        self.val_loader = val_load\n",
    "        \n",
    "        ##data file type (string name)\n",
    "        self.labeled_metadata = [data.metadata for data in labeled_data]\n",
    "        self.unlabeled_metadata = unlabeled_data.metadata\n",
    "        \n",
    "        self.genes = unlabeled_data.yIDs\n",
    "        \n",
    "        ##number of genes \n",
    "        x_dim = self.test_loader.dataset.get_dim()\n",
    "        \n",
    "        ##Feed forward neural net\n",
    "        self.init_model(x_dim, hid_dim_1, hid_dim_2, p_drop, params.device)\n",
    "        \n",
    "        self.n_clusters = n_clusters\n",
    "        \n",
    "        self.device = params.device\n",
    "        self.epochs = params.epochs\n",
    "        self.epochs_pretrain = params.epochs_pretrain\n",
    "        self.pretrain_flag = params.pretrain\n",
    "        self.model_file = params.model_file\n",
    "        self.lr = params.learning_rate\n",
    "        self.lr_gamma = params.lr_scheduler_gamma\n",
    "        self.step_size = params.lr_scheduler_step\n",
    "        \n",
    "        self.tau = tau\n",
    "        \n",
    "    ###################################################################\n",
    "    ###################################################################    \n",
    "    ###### With the fine tuned hyper parameter     \n",
    "    ###### Change the implementation of AE to VAE     \n",
    "    def init_model(self, x_dim, hid_dim, z_dim, p_drop, device):\n",
    "        \"\"\"\n",
    "        Initialize the model.\n",
    "        \"\"\"\n",
    "        self.model = FullNet(x_dim, hid_dim, z_dim, p_drop).to(device)\n",
    "    ###################################################################\n",
    "    ###################################################################\n",
    "    \n",
    "    def init_optim(self, param1, param2, learning_rate):\n",
    "        \"\"\"Initializing optimizers.\"\"\"\n",
    "        \n",
    "        optim = torch.optim.Adam(params=param1, lr=learning_rate)\n",
    "        optim_landmk_test = torch.optim.Adam(params=param2, lr=learning_rate)\n",
    "        \n",
    "        return optim, optim_landmk_test\n",
    "    \n",
    "    ###################################################################\n",
    "    ###################################################################\n",
    "    ########Will need to alter this part for VAE (KL + reconstruction loss + regularized)\n",
    "    def pretrain(self, optim):\n",
    "        \"\"\"\n",
    "        Pretraining model with autoencoder.\n",
    "        optim: optimizer\n",
    "        \"\"\"\n",
    "        print('Pretraining..')\n",
    "        \n",
    "        for _ in range(self.epochs_pretrain):\n",
    "            for _, batch in enumerate(self.pretrain_loader):\n",
    "                x,_,_ = batch\n",
    "                x = x.to(self.device)\n",
    "                _, decoded = self.model(x)\n",
    "                loss = reconstruction_loss(decoded, x) \n",
    "                optim.zero_grad()              \n",
    "                loss.backward()                    \n",
    "                optim.step() \n",
    "    ###################################################################\n",
    "    ###################################################################\n",
    "    \n",
    "    \n",
    "    def train(self, evaluation_mode=True, save_all_embeddings=True):\n",
    "        \"\"\"Train model.\n",
    "        evaluation_mode: if True, validates model on the unlabeled dataset. \n",
    "        In the evaluation mode, ground truth labels of the unlabeled dataset must be \n",
    "        provided to validate model\n",
    "        \n",
    "        save_all_embeddings: if True, MARS embeddings for annotated and unannotated \n",
    "        experiments will be saved in an anndata object,\n",
    "        otherwise only unnanotated will be saved. \n",
    "        If naming is called after, all embeddings need to be saved\n",
    "        \n",
    "        return: adata: anndata object containing labeled and unlabeled meta-dataset \n",
    "        with MARS embeddings and estimated labels on the unlabeled dataset\n",
    "                landmk_all: landmarks of the labeled and unlabeled meta-dataset in the \n",
    "                order given for training. Landmarks on the unlabeled\n",
    "                            dataset are provided last\n",
    "                metrics: clustering metrics if evaluation_mode is True\n",
    "                \n",
    "        \"\"\"\n",
    "        tr_iter = [iter(dl) for dl in self.train_loader]\n",
    "        \n",
    "        if self.val_loader is not None:\n",
    "            val_iter = [iter(dl) for dl in self.val_loader]\n",
    "        \n",
    "        ####Pre train step \n",
    "        optim_pretrain = torch.optim.Adam(params=list(self.model.parameters()), lr=self.lr)\n",
    "        if self.pretrain_flag:\n",
    "            self.pretrain(optim_pretrain)\n",
    "        else:\n",
    "            self.model.load_state_dict(torch.load(self.MODEL_FILE))    \n",
    "        ####\n",
    "        \n",
    "        test_iter = iter(self.test_loader)\n",
    "        \n",
    "        landmk_tr, landmk_test = init_landmarks(self.n_clusters, \n",
    "                                                self.train_loader, \n",
    "                                                self.test_loader, \n",
    "                                                self.model, self.device)\n",
    "        \n",
    "        optim, optim_landmk_test = self.init_optim(list(self.model.encoder.parameters()), \n",
    "                                                   landmk_test, self.lr)\n",
    "        \n",
    "        lr_scheduler = torch.optim.lr_scheduler.StepLR(optimizer=optim,\n",
    "                                                       gamma=self.lr_gamma,\n",
    "                                                       step_size=self.step_size)\n",
    "        \n",
    "        best_acc = 0\n",
    "        for epoch in range(1, self.epochs+1):\n",
    "            \n",
    "            ##Set model for training\n",
    "            self.model.train()\n",
    "            \n",
    "            ##This is equivalent to train step \n",
    "            loss_tr, acc_tr, landmk_tr, landmk_test = self.do_epoch(tr_iter, \n",
    "                                                                    test_iter,\n",
    "                                                                    optim, \n",
    "                                                                    optim_landmk_test,\n",
    "                                                                    landmk_tr, \n",
    "                                                                    landmk_test)\n",
    "            if epoch==self.epochs: \n",
    "                print('\\n=== Epoch: {} ==='.format(epoch))\n",
    "                print('Train acc: {}'.format(acc_tr))\n",
    "            if self.val_loader is None:\n",
    "                continue\n",
    "            self.model.eval()\n",
    "            ##Stop training \n",
    "            \n",
    "            with torch.no_grad():\n",
    "                loss_val,acc_val = self.do_val_epoch(val_iter, landmk_tr)\n",
    "                if acc_val > best_acc:\n",
    "                    print('Saving model...')\n",
    "                    best_acc = acc_val\n",
    "                    best_state = self.model.state_dict()\n",
    "                    #torch.save(model.state_dict(), self.model_file)\n",
    "                postfix = ' (Best)' if acc_val >= best_acc else ' (Best: {})'.format(best_acc)\n",
    "                print('Val loss: {}, acc: {}{}'.format(loss_val, acc_val, postfix))\n",
    "            lr_scheduler.step()\n",
    "            \n",
    "        if self.val_loader is None:\n",
    "            best_state = self.model.state_dict() # best is last\n",
    "        \n",
    "        landmk_all = landmk_tr+[torch.stack(landmk_test).squeeze()]\n",
    "        \n",
    "        ##Test time (assign labels to unlabaled data)\n",
    "        adata_test, eval_results = self.assign_labels(landmk_all[-1], evaluation_mode)\n",
    "        \n",
    "        adata = self.save_result(tr_iter, adata_test, save_all_embeddings)\n",
    "        \n",
    "        if evaluation_mode:\n",
    "            return adata, landmk_all, eval_results\n",
    "        \n",
    "        return adata, landmk_all\n",
    "    \n",
    "    def save_result(self, tr_iter, adata_test, save_all_embeddings):\n",
    "        \"\"\"Saving embeddings from labeled and unlabeled dataset, ground truth labels and \n",
    "        predictions to joint anndata object.\"\"\"\n",
    "        adata_all = []\n",
    "\n",
    "        if save_all_embeddings:\n",
    "            for task in range(len(tr_iter)): # saving embeddings from labeled dataset\n",
    "                task = int(task)\n",
    "                x, y, cells = next(tr_iter[task])\n",
    "                x, y = x.to(self.device), y.to(self.device)\n",
    "                encoded,_ = self.model(x)\n",
    "                adata_all.append(self.pack_anndata(x, cells, encoded, gtruth=y))\n",
    "            \n",
    "        adata_all.append(adata_test)    \n",
    "        \n",
    "        if save_all_embeddings:\n",
    "            adata = adata_all[0].concatenate(adata_all[1:], \n",
    "                                             batch_key='experiment',\n",
    "                                             batch_categories=self.labeled_metadata+[self.unlabeled_metadata])\n",
    "        else:\n",
    "            adata = adata_all[0]\n",
    "\n",
    "            \n",
    "        adata.obsm['MARS_embedding'] = np.concatenate([a.uns['MARS_embedding'] for a in adata_all])\n",
    "        #adata.write('result_adata.h5ad')\n",
    "        \n",
    "        return adata\n",
    "    \n",
    "    def assign_labels(self, landmk_test, evaluation_mode):\n",
    "        \"\"\"Assigning cluster labels to the unlabeled meta-dataset.\n",
    "        test_iter: iterator over unlabeled dataset\n",
    "        landmk_test: landmarks in the unlabeled dataset\n",
    "        evaluation mode: computes clustering metrics if True\n",
    "        \"\"\"\n",
    "        #test_iter = iter(self.test_loader)\n",
    "            \n",
    "        torch.no_grad()\n",
    "        self.model.eval() # eval mode\n",
    "        \n",
    "        test_iter = iter(self.test_loader)\n",
    "        \n",
    "        x_test, y_true, cells = next(test_iter) # cells are needed because dataset is in random order\n",
    "        x_test = x_test.to(self.device)\n",
    "        \n",
    "        encoded_test,_ = self.model(x_test)\n",
    "        \n",
    "        ###Embedding space eucledian distance \n",
    "        dists = euclidean_dist(encoded_test, landmk_test)\n",
    "        \n",
    "        ###Prediction based on the minimal distance to learned landmark\n",
    "        y_pred = torch.min(dists, 1)[1]\n",
    "        \n",
    "        adata = self.pack_anndata(x_test, cells, encoded_test, y_true, y_pred)\n",
    "        \n",
    "        eval_results = None\n",
    "        if evaluation_mode:\n",
    "            eval_results = compute_scores(y_true, y_pred)\n",
    "            \n",
    "        return adata, eval_results\n",
    "    \n",
    "    \n",
    "    def pack_anndata(self, x_input, cells, embedding, gtruth=[], estimated=[]):\n",
    "        \"\"\"Pack results in anndata object.\n",
    "        x_input: gene expressions in the input space\n",
    "        cells: cell identifiers\n",
    "        embedding: resulting embedding of x_test using MARS\n",
    "        landmk: MARS estimated landmarks\n",
    "        gtruth: ground truth labels if available (default: empty list)\n",
    "        estimated: MARS estimated clusters if available (default: empty list)\n",
    "        \"\"\"\n",
    "        adata = anndata.AnnData(x_input.data.cpu().numpy())\n",
    "        adata.obs_names = cells\n",
    "        adata.var_names = self.genes\n",
    "        if len(estimated)!=0:\n",
    "            adata.obs['MARS_labels'] = pd.Categorical(values=estimated.cpu().numpy())\n",
    "        if len(gtruth)!=0:\n",
    "            adata.obs['truth_labels'] = pd.Categorical(values=gtruth.cpu().numpy())\n",
    "        adata.uns['MARS_embedding'] = embedding.data.cpu().numpy()\n",
    "        \n",
    "        return adata\n",
    "    \n",
    "    \n",
    "    def do_epoch(self, tr_iter, test_iter, optim, optim_landmk_test, landmk_tr, landmk_test):\n",
    "        \"\"\"\n",
    "        One training epoch.\n",
    "        tr_iter: iterator over labeled meta-data\n",
    "        test_iter: iterator over unlabeled meta-data\n",
    "        \n",
    "        optim: optimizer for embedding\n",
    "        optim_landmk_test: optimizer for test landmarks\n",
    "        \n",
    "        landmk_tr: landmarks of labeled meta-data from previous epoch\n",
    "        landmk_test: landmarks of unlabeled meta-data from previous epoch\n",
    "        \"\"\"\n",
    "        self.set_requires_grad(False)\n",
    "        \n",
    "        for landmk in landmk_test:\n",
    "            landmk.requires_grad=False\n",
    "        \n",
    "        optim_landmk_test.zero_grad()\n",
    "        \n",
    "        # update centroids    \n",
    "        task_idx = torch.randperm(len(tr_iter)) ##shuffle per epoch\n",
    "        for task in task_idx:\n",
    "            \n",
    "            task = int(task)\n",
    "            x, y, _ = next(tr_iter[task])\n",
    "            x, y = x.to(self.device), y.to(self.device)\n",
    "            encoded,_ = self.model(x)\n",
    "            ##Compute how good the landmarks are to true labels\n",
    "            curr_landmk_tr = compute_landmarks_tr(encoded, y, landmk_tr[task], tau=self.tau)\n",
    "            landmk_tr[task] = curr_landmk_tr.data # save landmarks\n",
    "            \n",
    "        for landmk in landmk_test:\n",
    "            landmk.requires_grad=True\n",
    "            \n",
    "        x,y_test,_ = next(test_iter)\n",
    "        x = x.to(self.device)\n",
    "        encoded,_ = self.model(x)\n",
    "        loss, args_count = loss_test(encoded, \n",
    "                                     torch.stack(landmk_test).squeeze(), \n",
    "                                     self.tau)\n",
    "        loss.backward()\n",
    "        optim_landmk_test.step()\n",
    "                \n",
    "        # update embedding\n",
    "        self.set_requires_grad(True)\n",
    "        for landmk in landmk_test:\n",
    "            landmk.requires_grad=False\n",
    "            \n",
    "        optim.zero_grad()\n",
    "        total_accuracy = 0\n",
    "        total_loss = 0\n",
    "        ntasks = 0\n",
    "        mean_accuracy = 0\n",
    "        \n",
    "        task_idx = torch.randperm(len(tr_iter))\n",
    "        for task in task_idx:\n",
    "            task = int(task)\n",
    "            x, y, _ = next(tr_iter[task])\n",
    "            x, y = x.to(self.device), y.to(self.device)\n",
    "            encoded,_ = self.model(x)\n",
    "            ###Eucledian distance between embedding and landmark\n",
    "            loss, acc = loss_task(encoded, landmk_tr[task], y, criterion='dist')\n",
    "            total_loss += loss\n",
    "            total_accuracy += acc.item()\n",
    "            ntasks += 1\n",
    "        \n",
    "        if ntasks>0:\n",
    "            mean_accuracy = total_accuracy / ntasks\n",
    "        \n",
    "        # test part\n",
    "        x,y,_ = next(test_iter)\n",
    "        x = x.to(self.device)\n",
    "        encoded,_ = self.model(x)\n",
    "        loss,_ = loss_test(encoded, torch.stack(landmk_test).squeeze(), self.tau)\n",
    "        \n",
    "        total_loss += loss\n",
    "        ntasks += 1\n",
    "    \n",
    "        mean_loss = total_loss / ntasks\n",
    "        \n",
    "        mean_loss.backward()\n",
    "        optim.step()\n",
    "        \n",
    "        return mean_loss, mean_accuracy, landmk_tr, landmk_test\n",
    "    \n",
    "    def do_val_epoch(self, val_iter, prev_landmk):\n",
    "        \"\"\"One epoch of validation.\n",
    "        val_iter: iterator over validation set\n",
    "        prev_landmk: landmarks from previous epoch\n",
    "        \"\"\"\n",
    "        ntasks = len(val_iter)\n",
    "        task_idx = torch.randperm(ntasks)\n",
    "        \n",
    "        total_loss = 0\n",
    "        total_accuracy = 0\n",
    "        \n",
    "        for task in task_idx:\n",
    "            x, y, _ = next(val_iter[task])\n",
    "            x, y = x.to(self.device), y.to(self.device)\n",
    "            encoded = self.model(x)\n",
    "            loss, acc = loss_task(encoded, prev_landmk[task], y, criterion='dist')\n",
    "            total_loss += loss\n",
    "            total_accuracy += acc.item()\n",
    "        mean_accuracy = total_accuracy / ntasks\n",
    "        mean_loss = total_loss / ntasks\n",
    "        \n",
    "        return mean_loss, mean_accuracy\n",
    "    \n",
    "    \n",
    "    def set_requires_grad(self, requires_grad):\n",
    "        for param in self.model.parameters():\n",
    "            param.requires_grad = requires_grad\n",
    "    \n",
    "    def name_cell_types(self, adata, landmk_all, cell_name_mappings, \n",
    "                        top_match=5, umap_reduce_dim=True, ndim=10):\n",
    "        \"\"\"For each test cluster, estimate sigma and mean. \n",
    "        Fit Gaussian distribution with that mean and sigma\n",
    "        and calculate the probability of each of the train landmarks \n",
    "        to be the neighbor to the mean data point.\n",
    "        Normalization is performed with regards to all other landmarks in train.\"\"\"\n",
    "        \n",
    "        experiments = list(OrderedDict.fromkeys(list(adata.obs['experiment'])))\n",
    "        \n",
    "        encoded_tr = []\n",
    "        landmk_tr = []\n",
    "        landmk_tr_labels = []\n",
    "        for idx, exp in enumerate(experiments[:-1]):\n",
    "            tiss = adata[adata.obs['experiment'] == exp,:]\n",
    "            \n",
    "            if exp==self.unlabeled_metadata: \n",
    "                raise ValueError(\"Error: Unlabeled dataset needs to be last one in the input anndata object.\")\n",
    "                \n",
    "            encoded_tr.append(tiss.obsm['MARS_embedding'])\n",
    "            landmk_tr.append(landmk_all[idx])\n",
    "            landmk_tr_labels.append(np.unique(tiss.obs['truth_labels']))\n",
    "            \n",
    "        tiss = adata[adata.obs['experiment'] == self.unlabeled_metadata,:]\n",
    "        ypred_test = tiss.obs['MARS_labels']\n",
    "        uniq_ytest = np.unique(ypred_test)\n",
    "        encoded_test = tiss.obsm['MARS_embedding']\n",
    "        \n",
    "        landmk_tr_labels = np.concatenate(landmk_tr_labels)\n",
    "        encoded_tr = np.concatenate(encoded_tr)\n",
    "        landmk_tr = np.concatenate([p.cpu() for p in landmk_tr])\n",
    "        if  umap_reduce_dim:\n",
    "            encoded_extend = np.concatenate((encoded_tr, encoded_test, landmk_tr))\n",
    "            adata = anndata.AnnData(encoded_extend)\n",
    "            sc.pp.neighbors(adata, n_neighbors=15, use_rep='X')\n",
    "            sc.tl.umap(adata, n_components=ndim)\n",
    "            encoded_extend = adata.obsm['X_umap']\n",
    "            n1 = len(encoded_tr)\n",
    "            n2 = n1 + len(encoded_test)\n",
    "            encoded_tr = encoded_extend[:n1,:]\n",
    "            encoded_test = encoded_extend[n1:n2,:]\n",
    "            landmk_tr = encoded_extend[n2:,:]\n",
    "        \n",
    "        interp_names = defaultdict(list)\n",
    "        for ytest in uniq_ytest:\n",
    "            print('\\nCluster label: {}'.format(str(ytest)))\n",
    "            idx = np.where(ypred_test==ytest)\n",
    "            subset_encoded = encoded_test[idx[0],:]\n",
    "            mean = np.expand_dims(np.mean(subset_encoded, axis=0),0)\n",
    "            \n",
    "            sigma  = self.estimate_sigma(subset_encoded)\n",
    "            \n",
    "            prob = np.exp(-np.power(distance.cdist(mean, landmk_tr, metric='euclidean'),2)/(2*sigma*sigma))\n",
    "            prob = np.squeeze(prob, 0)\n",
    "            normalizat = np.sum(prob)\n",
    "            if normalizat==0:\n",
    "                print('Unassigned')\n",
    "                interp_names[ytest].append(\"unassigned\")\n",
    "                continue\n",
    "            \n",
    "            prob = np.divide(prob, normalizat)\n",
    "            \n",
    "            uniq_tr = np.unique(landmk_tr_labels)\n",
    "            prob_unique = []\n",
    "            for cell_type in uniq_tr: # sum probabilities of same landmarks\n",
    "                prob_unique.append(np.sum(prob[np.where(landmk_tr_labels==cell_type)]))\n",
    "            \n",
    "            sorted = np.argsort(prob_unique, axis=0)\n",
    "            best = uniq_tr[sorted[-top_match:]]\n",
    "            sortedv = np.sort(prob_unique, axis=0)\n",
    "            sortedv = sortedv[-top_match:]\n",
    "            for idx, b in enumerate(best):\n",
    "                interp_names[ytest].append((cell_name_mappings[b], sortedv[idx]))\n",
    "                print('{}: {}'.format(cell_name_mappings[b], sortedv[idx]))\n",
    "                \n",
    "        return interp_names\n",
    "    \n",
    "    \n",
    "    def estimate_sigma(self, dataset):\n",
    "        nex = dataset.shape[0]\n",
    "        dst = []\n",
    "        for i in range(nex):\n",
    "            for j in range(i+1, nex):\n",
    "                dst.append(distance.euclidean(dataset[i,:],dataset[j,:]))\n",
    "        return np.std(dst)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dynamic-trace",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import silhouette_samples, silhouette_score\n",
    "import matplotlib.cm as cm\n",
    "\n",
    "def plot_silhouette(X, y, n_clusters, title=None, save_fn=None):\n",
    "    \"\"\"\n",
    "    Calculates the silhouette coefficient and plots the graph\n",
    "    \n",
    "    \"\"\"\n",
    "    \n",
    "    sil = silhouette_samples(X, y)\n",
    "    sil_avg = silhouette_score(X, y)\n",
    "    \n",
    "    fig, ax = plt.subplots()\n",
    "    ax.set_xlim([-0.1,1])\n",
    "    ax.set_ylim([0, len(X)+(n_clusters+1)*10])\n",
    "\n",
    "    y_lower = 10\n",
    "\n",
    "    for i in range(n_clusters):\n",
    "        # Aggregate the silhouette scores for samples belonging to\n",
    "        # cluster i, and sort them\n",
    "    #     ith_cluster_silhouette_values = sil[np.array(adata_mars.obs['MARS_labels']) == i]\n",
    "        ith_cluster_silhouette_values = sil[y == i]\n",
    "\n",
    "        ith_cluster_silhouette_values.sort()\n",
    "\n",
    "        size_cluster_i = ith_cluster_silhouette_values.shape[0]\n",
    "        y_upper = y_lower + size_cluster_i\n",
    "\n",
    "        color = cm.nipy_spectral(float(i) / n_clusters)\n",
    "        ax.fill_betweenx(np.arange(y_lower, y_upper),\n",
    "                          0, ith_cluster_silhouette_values,\n",
    "                          facecolor=color, edgecolor=color, alpha=0.7)\n",
    "\n",
    "        # Label the silhouette plots with their cluster numbers at the middle\n",
    "        ax.text(-0.05, y_lower + 0.5 * size_cluster_i, str(i))\n",
    "\n",
    "        # Compute the new y_lower for next plot\n",
    "        y_lower = y_upper + 10  # 10 for the 0 samples\n",
    "\n",
    "    if title == None:\n",
    "        title = \"The silhouette plot for the various clusters.\"\n",
    "    \n",
    "    ax.set_title(title)\n",
    "    ax.set_xlabel(\"Silhouette coefficient values\")\n",
    "    ax.set_ylabel(\"Cluster label\")\n",
    "\n",
    "    ax.text(0.7, 0.1,'Avg. Silhouette \\nScore: {:.4f}'.format(sil_avg),\n",
    "         horizontalalignment='left', verticalalignment='center',\n",
    "         transform = ax.transAxes, weight='bold')\n",
    "    \n",
    "    # The vertical line for average silhouette score of all the values\n",
    "    ax.axvline(x=sil_avg, color=\"red\", linestyle=\"--\")\n",
    "\n",
    "    ax.set_yticks([])  # Clear the yaxis labels / ticks\n",
    "    ax.set_xticks([-0.1, 0, 0.2, 0.4, 0.6, 0.8, 1]);\n",
    "    \n",
    "    if save_fn != None:\n",
    "        plt.savefig(save_fn)\n",
    "    \n",
    "    \n",
    "#Appends the MARS labels to the scipy object\n",
    "def label_train_data(x):\n",
    "    label = x[1]\n",
    "    if(np.isnan(x[1])):\n",
    "        label=x[0]\n",
    "    return label\n",
    "\n",
    "def add_MARS_labels_to_scObject(adata, df):\n",
    "    \"\"\"\n",
    "    Appends the MARS labels to the Scanpy object\n",
    "    adata - MARS ouput\n",
    "    df - original scanpy object \n",
    "    \"\"\"\n",
    "    \n",
    "    temp = adata.obs.copy()\n",
    "    temp['MARS_label'] = temp[['truth_labels', 'MARS_labels']].apply(label_train_data, axis=1)\n",
    "    temp['MARS_label'] = temp['MARS_label'].astype('category')\n",
    "    idx_list = list(temp.index)\n",
    "    idx_list = [x.split('-')[0] for x in idx_list]\n",
    "    temp.index = np.array(idx_list)\n",
    "    df.obs = df.obs.join(temp['MARS_label'], how='outer')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "chronic-stations",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "params, unknown= get_parser().parse_known_args()\n",
    "params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "stone-advancement",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "if torch.cuda.is_available() and not params.cuda:\n",
    "        print(\"WARNING: You have a CUDA device, so you should probably run with --cuda\")\n",
    "device = 'cuda:0' if torch.cuda.is_available() and params.cuda else 'cpu'\n",
    "params.device = device"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "stone-oliver",
   "metadata": {},
   "source": [
    "### Loads in Data and Visualizes it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "industrial-booking",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Loads in the preprocessed Table 5 dataset\n",
    "df = sc.read_h5ad('Table5_processed.h5ad')\n",
    "\n",
    "#Isolates the Connective Tissue\n",
    "control_idx = (df.obs['time.point'] == '2kA') | (df.obs['time.point'] == '2kB')\n",
    "control = df[control_idx]\n",
    "control.obs['cell.type'] = pd.Categorical(control.obs['cell.type'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "respective-quebec",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Adds labels to data\n",
    "healthy_dict = {1:'fCT1',\n",
    "                5:'fCT2',\n",
    "                0:'fCT3',\n",
    "                2:'fCT4',\n",
    "                6:'fCT5',\n",
    "                4:'Periskeletal Cells',\n",
    "                3:'Tenocytes',\n",
    "                7:'Cycling cells',\n",
    "                'fCT1':1,\n",
    "                'fCT2':5,\n",
    "                'fCT3':0,\n",
    "                'fCT4':2,\n",
    "                'fCT5':6,\n",
    "                'Periskeletal Cells':4,\n",
    "                'Tenocytes':3,\n",
    "                'Cycling cells':7}\n",
    "\n",
    "control.obs['cell.label'] = control.obs['cell.type'].apply(lambda x: healthy_dict[x])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "popular-turning",
   "metadata": {},
   "outputs": [],
   "source": [
    "sc.set_figure_params(dpi=80, dpi_save=300, frameon=True, vector_friendly=True, fontsize=14, figsize=(8,6), color_map=None, format='pdf', facecolor=None, transparent=False, ipython_format='png2x')\n",
    "\n",
    "#Visualizes the data\n",
    "sc.pp.neighbors(control, n_neighbors=30, use_rep='X')\n",
    "sc.pp.pca(control, n_comps=50)\n",
    "sc.tl.tsne(control)\n",
    "sc.pl.tsne(control, color=['time.point','cell.type'],size=50)\n",
    "# sc.pl.tsne(control, color=['cell.label'],size=100,\n",
    "#           title='TSNE of Healthy Axolotl Connective Tissue',\n",
    "#           save='Control TSNE.jpg')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "elect-present",
   "metadata": {},
   "outputs": [],
   "source": [
    "# t9 = sc.read_h5ad('Table9_18dpa_processed.h5ad')\n",
    "\n",
    "# sc.pp.highly_variable_genes(t9, min_mean=0.0125, max_mean=3, min_disp=0.5)\n",
    "# t9 = t9[:, t9.var.highly_variable]\n",
    "# sc.pp.neighbors(t9, n_neighbors=30, use_rep='X')\n",
    "# sc.pp.pca(t9, n_comps=50)\n",
    "# sc.tl.tsne(t9)\n",
    "sc.tl.leiden(t9)\n",
    "sc.pl.tsne(t9,size=30, color='leiden',\n",
    "          title='TSNE of Axolotl Connective Tissue 18 Days Post-Amputation',\n",
    "          save='18dpa TSNE.jpg')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "nominated-temple",
   "metadata": {},
   "outputs": [],
   "source": [
    "t9 = sc.read_h5ad('Table9_25dpa_processed.h5ad')\n",
    "\n",
    "sc.pp.highly_variable_genes(t9, min_mean=0.0125, max_mean=3, min_disp=0.5)\n",
    "t9 = t9[:, t9.var.highly_variable]\n",
    "sc.pp.neighbors(t9, n_neighbors=30, use_rep='X')\n",
    "sc.pp.pca(t9, n_comps=50)\n",
    "sc.tl.tsne(t9)\n",
    "sc.tl.leiden(t9)\n",
    "sc.pl.tsne(t9,size=30, color='leiden',\n",
    "          title='TSNE of Axolotl Connective Tissue 25 Days Post-Amputation',\n",
    "          save='25dpa TSNE.jpg')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "normal-sharing",
   "metadata": {},
   "outputs": [],
   "source": [
    "t9 = sc.read_h5ad('Table9_38dpa_processed.h5ad')\n",
    "\n",
    "sc.pp.highly_variable_genes(t9, min_mean=0.0125, max_mean=3, min_disp=0.5)\n",
    "t9 = t9[:, t9.var.highly_variable]\n",
    "sc.pp.neighbors(t9, n_neighbors=30, use_rep='X')\n",
    "sc.pp.pca(t9, n_comps=50)\n",
    "sc.tl.tsne(t9)\n",
    "sc.tl.leiden(t9)\n",
    "sc.pl.tsne(t9,size=30, color='leiden',\n",
    "          title='TSNE of Axolotl Connective Tissue 38 Days Post-Amputation',\n",
    "          save='38dpa TSNE.jpg')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "advanced-disney",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "pressing-think",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "standard-driving",
   "metadata": {},
   "source": [
    "### Train and test MARS Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "technical-exhibition",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_annotated = np.array(control.obs['cell.type'])\n",
    "# healthy_dict = {'fCT1':1,\n",
    "#                'fCT2':5,\n",
    "#                'fCT3':0,\n",
    "#                'fCT4':2,\n",
    "#                'fCT5':6,\n",
    "#                'Periskeletal Cells':4,\n",
    "#                'Tenocytes':3,\n",
    "#                'Cycling cells':7}\n",
    "# y_annotated = np.array([healthy_dict[y] for y in y_annotated])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "optimum-venice",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Splits the healthy control into train and test sets\n",
    "\n",
    "# x_train = control[:1800]\n",
    "# x_test = control[1800:]\n",
    "# y_train = y_annotated[:1800]\n",
    "# y_test = y_annotated[1800:]\n",
    "\n",
    "x_train, x_test, y_train, y_test = train_test_split(\n",
    "    control, y_annotated, test_size=0.2, random_state=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "noble-tucson",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Runs MARS\n",
    "\n",
    "# Splits the healthy control into train and test sets\n",
    "x_train, x_test, y_train, y_test = train_test_split(\n",
    "    control, y_annotated, test_size=0.3, random_state=0)\n",
    "\n",
    "# Sets up datasets\n",
    "x_annotated = ExperimentDataset(x_train.X.toarray(), \n",
    "                                x_train.obs_names, \n",
    "                                x_train.var_names, \n",
    "                                'Training', \n",
    "                                y_train)\n",
    "x_unannotated = ExperimentDataset(x_test.X.toarray(), \n",
    "                                  x_test.obs_names, \n",
    "                                  x_test.var_names, \n",
    "                                  'Testing', \n",
    "                                  y_test)\n",
    "pretrain_data = ExperimentDataset(x_test.X.toarray(), \n",
    "                                  x_test.obs_names, \n",
    "                                  x_test.var_names, \n",
    "                                  'Testing')\n",
    "n_clusters = len(np.unique(x_unannotated.y))\n",
    "\n",
    "val_split=0.8\n",
    "train_load, test_load, pretrain_load, val_load = init_data_loaders([x_annotated], \n",
    "                                                                   x_unannotated, \n",
    "                                                                   pretrain_data, \n",
    "                                                                   params.pretrain_batch, \n",
    "                                                                   val_split)\n",
    "#Initializes MARS\n",
    "mars = MARS(n_clusters, params, \n",
    "            [x_annotated], x_unannotated, \n",
    "            pretrain_data, hid_dim_1=1000, hid_dim_2=100)\n",
    "#Runs MARS in evaluation mode\n",
    "adata, landmarks, scores = mars.train(evaluation_mode=True, save_all_embeddings=False)\n",
    "\n",
    "scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fuzzy-instrumentation",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Creates AnnData object using MARS embeddings as X\n",
    "adata_mars = AnnData(adata.obsm['MARS_embedding'])\n",
    "adata_mars.obs['MARS_labels'] = pd.Categorical(adata.obs['MARS_labels'])\n",
    "adata_mars.obs['ground_truth'] = pd.Categorical(adata.obs['truth_labels'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "experimental-insurance",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.shape(adata_mars.X) # 100-dimensional MARS embeddings space"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "irish-professor",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Fills in the gaps, only if save_all_embedding is true\n",
    "adata_mars.obs['MARS_labels'] = adata_mars.obs[['ground_truth', 'MARS_labels']].apply(label_train_data, axis=1)\n",
    "adata_mars.obs['MARS_labels'] = adata_mars.obs['MARS_labels'].astype('category')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "square-camera",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Visualizes the unannotated dataset\n",
    "#Note that it is not expected that the numerical labels match\n",
    "# sc.pp.neighbors(adata_mars, n_neighbors=30, use_rep='X')\n",
    "# sc.tl.umap(adata_mars)\n",
    "sc.pl.umap(adata_mars, color=['ground_truth','MARS_labels'],size=50,\n",
    "          title=['Seurat Labels','MARS Labels'],\n",
    "          save=\"MARS_latent_control.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "alive-official",
   "metadata": {},
   "outputs": [],
   "source": [
    "import itertools as it\n",
    "def grid_search(n_clusters, hd2, num_batches):\n",
    "    \"\"\"\n",
    "    perform a grid search over different hyper parameter configurations\n",
    "    to determing the optimal configuration for the cnn\n",
    "    \"\"\"\n",
    "    accuracy = []\n",
    "    hyperparam_combos = list(\n",
    "        it.product(n_clusters, hd2))\n",
    "    \n",
    "    for n_clusters, hd2 in hyperparam_combos:\n",
    "        \n",
    "        best_acc = 0;\n",
    "        \n",
    "        for n in range(num_batches):\n",
    "            # Splits the healthy control into train and test sets\n",
    "            x_train, x_test, y_train, y_test = train_test_split(\n",
    "                control, y_annotated, test_size=0.3, random_state=0)\n",
    "\n",
    "            # Sets up datasets\n",
    "            x_annotated = ExperimentDataset(x_train.X.toarray(), \n",
    "                                            x_train.obs_names, \n",
    "                                            x_train.var_names, \n",
    "                                            'Training', \n",
    "                                            y_train)\n",
    "            x_unannotated = ExperimentDataset(x_test.X.toarray(), \n",
    "                                              x_test.obs_names, \n",
    "                                              x_test.var_names, \n",
    "                                              'Testing', \n",
    "                                              y_test)\n",
    "            pretrain_data = ExperimentDataset(x_test.X.toarray(), \n",
    "                                              x_test.obs_names, \n",
    "                                              x_test.var_names, \n",
    "                                              'Testing')\n",
    "            val_split=0.8\n",
    "            train_load, test_load, pretrain_load, val_load = init_data_loaders([x_annotated], \n",
    "                                                                               x_unannotated, \n",
    "                                                                               pretrain_data, \n",
    "                                                                               params.pretrain_batch, \n",
    "                                                                               val_split)\n",
    "            #Initializes MARS\n",
    "            mars = MARS(n_clusters, params, \n",
    "                        [x_annotated], x_unannotated, \n",
    "                        pretrain_data, hid_dim_1=1000, hid_dim_2=hd2)\n",
    "            #Runs MARS in evaluation mode\n",
    "            adata, landmarks, scores = mars.train(evaluation_mode=True, save_all_embeddings=False)\n",
    "            \n",
    "            if best_acc < scores['accuracy']:\n",
    "                best_acc = scores['accuracy']\n",
    "            \n",
    "        accuracy.append([n_clusters, hd2, best_acc])\n",
    "        \n",
    "    df = pd.DataFrame(accuracy, \n",
    "                             columns=['Number of clusters','Latent dimensions','accuracy'])                                              \n",
    "    return df\n",
    "\n",
    "\n",
    "# num_clust = [6,7,8,9,10]\n",
    "num_clust = [8,9,10]\n",
    "hidden_dim_2 = [20,30,50,80,100,200]\n",
    "n_batches = 5\n",
    "acc_df = grid_search(num_clust, hidden_dim_2, n_batches)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "convinced-shepherd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# display table of results\n",
    "grid_search_pivot = (acc_df.pivot_table(values=['accuracy'],\n",
    "                                        columns=['Number of clusters'],\n",
    "                                        index=['Latent dimensions']))\n",
    "grid_search_pivot.style.format('{:.3f}').background_gradient(cmap='magma_r',\n",
    "                                                             axis=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "frequent-stationery",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Generates the silhouette plot\n",
    "#if save_all_embedding = True, makes sure you only plot the test set\n",
    "plot_silhouette(adata_mars.X[:],np.array(adata_mars.obs['ground_truth'][:]), n_clusters,\n",
    "                title='Seurat Labels', save_fn='Seural_Silhouette.jpg')\n",
    "plot_silhouette(adata_mars.X[:],np.array(adata_mars.obs['MARS_labels'][:]), n_clusters,\n",
    "                title='MARS Labels', save_fn='MARS_Silhouette.jpg')\n",
    "\n",
    "\n",
    "# plot_silhouette(adata_mars.X[len(y_train):],np.array(adata_mars.obs['ground_truth'][len(y_train):]), n_clusters,\n",
    "#                 title='Seurat Labels', save_fn='Seural_Test_Silhouette.jpg')\n",
    "# plot_silhouette(adata_mars.X[len(y_train):],np.array(adata_mars.obs['MARS_labels'][len(y_train):]), n_clusters,\n",
    "#                 title='MARS Labels', save_fn='MARS_Test_Silhouette.jpg')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bored-weight",
   "metadata": {},
   "outputs": [],
   "source": [
    "add_MARS_labels_to_scObject(adata, control)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "certain-dress",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Look at marker genes for found clusters\n",
    "sc.tl.rank_genes_groups(control, 'MARS', method='t-test')\n",
    "sc.pl.rank_genes_groups(control, n_genes=25, sharey=False, save='Control_MARS_ranked_genes.png')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "thirty-luxembourg",
   "metadata": {},
   "outputs": [],
   "source": [
    "sc.tl.rank_genes_groups(control, 'cell.type', method='t-test')\n",
    "sc.pl.rank_genes_groups(control, n_genes=25, sharey=False, save='Control_cell_type_ranked_genes.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "automatic-discharge",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "joined-holmes",
   "metadata": {},
   "source": [
    "### Applying MARS to label Amputation Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "wrapped-chase",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Using the merged and scaled data\n",
    "# df = sc.read_h5ad(\"Scaled_merged_data.h5ad\")\n",
    "# control_idx = (df.obs['time.point'] == 'CTa') | (df.obs['time.point'] == 'CTb') | (df.obs['time.point'] == '2kA') | (df.obs['time.point'] == '2kB')\n",
    "# amp_idx = (df.obs['time.point'] != 'CTa') & (df.obs['time.point'] != 'CTb') & (df.obs['time.point'] != '2kA') & (df.obs['time.point'] != '2kB')\n",
    "# control = df[control_idx]\n",
    "# amp = df[amp_idx]\n",
    "\n",
    "#Use individually processed data\n",
    "healthy = sc.read_h5ad('Table5_processed.h5ad')\n",
    "h_vars = healthy.var_names\n",
    "amp18 = sc.read_h5ad('Table9_18dpa_processed.h5ad')\n",
    "amp18_vars = amp18.var_names\n",
    "amp25 = sc.read_h5ad('Table9_25dpa_processed.h5ad')\n",
    "amp25_vars = amp25.var_names\n",
    "amp38 = sc.read_h5ad('Table9_38dpa_processed.h5ad')\n",
    "amp38_vars = amp38.var_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "approximate-lunch",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Identified intersection of features\n",
    "healthy_18 = healthy[:,list(set(h_vars) & set(amp18_vars))]\n",
    "amp18_ = amp18[:,list(set(h_vars) & set(amp18_vars))]\n",
    "\n",
    "healthy_25 = healthy[:,list(set(h_vars) & set(amp25_vars))]\n",
    "amp25_ = amp25[:,list(set(h_vars) & set(amp25_vars))]\n",
    "\n",
    "healthy_38 = healthy[:,list(set(h_vars) & set(amp38_vars))]\n",
    "amp38_ = amp38[:,list(set(h_vars) & set(amp38_vars))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "sweet-determination",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_MARS(control, amp, n_clusters=None, save_all_embed=True):\n",
    "    \"\"\"\n",
    "    Runs MARS, taking in an annotated and unannotated sample\n",
    "    Processes the result and returns an AnnData and landmark\n",
    "    \n",
    "    \"\"\"\n",
    "    #Creates the ExperimentDataset object using the annotated and \n",
    "    #unannontated samples\n",
    "    y_annotated = np.array(control.obs['cell.type'])\n",
    "    annotated = ExperimentDataset(control.X, \n",
    "                              control.obs_names, \n",
    "                              control.var_names, \n",
    "                              'HealthyCells', \n",
    "                              y_annotated)\n",
    "    unannotated = ExperimentDataset(amp.X,\n",
    "                                amp.obs_names, \n",
    "                                amp.var_names, \n",
    "                                'Amputations',\n",
    "                                amp.obs['cell.type'])\n",
    "    pretrain_data = ExperimentDataset(amp.X, \n",
    "                                  amp.obs_names, \n",
    "                                  amp.var_names,\n",
    "                                  'Amputations')\n",
    "    \n",
    "    #If n_cluster not passed in, the n\n",
    "    if n_clusters == None:\n",
    "        n_clusters = len(np.unique(unannotated.y))\n",
    "    \n",
    "    val_split = 0.8 #1 #by default because we are not training\n",
    "    train_load, test_load, pretrain_load, val_load = init_data_loaders([annotated], \n",
    "                                                                   unannotated, \n",
    "                                                                   pretrain_data, \n",
    "                                                                   params.pretrain_batch, \n",
    "                                                                   val_split)\n",
    "    #Creates MARS instance\n",
    "    mars = MARS(n_clusters, params, \n",
    "            [annotated], unannotated, \n",
    "            pretrain_data, hid_dim_1=1000, hid_dim_2=100)\n",
    "    \n",
    "    adata, landmarks = mars.train(evaluation_mode=False, \n",
    "                                      save_all_embeddings=save_all_embed)\n",
    "    \n",
    "#     return adata, landmarks\n",
    "    \n",
    "    adata_mars = AnnData(adata.obsm['MARS_embedding'])\n",
    "    adata_mars.obs['MARS_labels'] = pd.Categorical(adata.obs['MARS_labels'])\n",
    "    adata_mars.obs['ground_truth'] = pd.Categorical(adata.obs['truth_labels'])\n",
    "    \n",
    "    #Fills in the gaps, only if save_all_embedding is true\n",
    "    if save_all_embed:\n",
    "        adata_mars.obs['MARS_labels'] = adata_mars.obs[['ground_truth', 'MARS_labels']].apply(label_train_data, axis=1)\n",
    "        adata_mars.obs['MARS_labels'] = adata_mars.obs['MARS_labels'].astype('category')\n",
    "\n",
    "    return adata_mars, landmarks\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "statutory-classics",
   "metadata": {},
   "outputs": [],
   "source": [
    "# adata18, landmarks = run_MARS(healthy_18, amp18_, n_clusters=None, save_all_embed=False)\n",
    "adata25, landmarks = run_MARS(healthy_25, amp25_, n_clusters=None, save_all_embed=False)\n",
    "# adata38, landmarks = run_MARS(healthy_38, amp38_, n_clusters=None, save_all_embed=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "retired-election",
   "metadata": {},
   "outputs": [],
   "source": [
    "sc.pp.neighbors(adata18, n_neighbors=30, use_rep='X')\n",
    "sc.tl.umap(adata18)\n",
    "sc.pl.umap(adata18, color=['ground_truth','MARS_labels'],size=50,\n",
    "           title=['Seurat Labels','MARS Labels'], save='MARS 18dpa.jpg')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "precious-victor",
   "metadata": {},
   "outputs": [],
   "source": [
    "sc.pp.neighbors(adata25, n_neighbors=30, use_rep='X')\n",
    "sc.tl.umap(adata25)\n",
    "sc.pl.umap(adata25, color=['ground_truth','MARS_labels'],size=50,\n",
    "                     title=['Seurat Labels','MARS Labels'], save='MARS 25dpa.jpg')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "meaningful-beatles",
   "metadata": {},
   "outputs": [],
   "source": [
    "sc.pp.neighbors(adata38, n_neighbors=30, use_rep='X')\n",
    "sc.tl.umap(adata38)\n",
    "sc.pl.umap(adata38, color=['ground_truth','MARS_labels'],size=50,\n",
    "                     title=['Seurat Labels','MARS Labels'], save='MARS 38dpa.jpg')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "rolled-bangladesh",
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_anndata(adata, mars_adata):\n",
    "    \"\"\"\n",
    "    Updates the original AnnData file with the MARS labels\n",
    "    as well as the UMAP projection using the latent space\n",
    "    Inputs:\n",
    "        adata - original AnnData file\n",
    "        mars_adata - result from MARS model, must perform UMAP first!\n",
    "    \"\"\"\n",
    "    #Appends the MARS label to the original \n",
    "    adata.obs['MARS_labels'] = list(map(str, mars_adata.obs['MARS_labels']))\n",
    "    #Adds the UMAP info to original AnnData\n",
    "    adata.obsm['X_umap'] = mars_adata.obsm['X_umap']\n",
    "    adata.obsp = mars_adata.obsp\n",
    "    adata.uns = mars_adata.uns\n",
    "\n",
    "    return adata\n",
    "\n",
    "# amp18 = update_anndata(amp18, adata18)\n",
    "# amp25 = update_anndata(amp25, adata25)\n",
    "# amp38 = update_anndata(amp38, adata38)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "other-attendance",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Finds marker genes for each cluster\n",
    "sc.tl.rank_genes_groups(amp18, 'MARS_labels', method='t-test')\n",
    "sc.tl.rank_genes_groups(amp25, 'MARS_labels', method='t-test')\n",
    "sc.tl.rank_genes_groups(amp38, 'MARS_labels', method='t-test')\n",
    "\n",
    "# sc.pl.rank_genes_groups(amp18, n_genes=25, sharey=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "continuous-ranch",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame(amp18.uns['rank_genes_groups']['names']).head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "blocked-musical",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame(amp25.uns['rank_genes_groups']['names']).head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dried-million",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame(amp38.uns['rank_genes_groups']['names']).head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "seven-exhibit",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "inappropriate-colon",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "separated-medicare",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "sudden-circumstances",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "gothic-black",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "loved-penetration",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "imperial-hammer",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "suited-mystery",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
